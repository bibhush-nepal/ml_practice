{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deep Learning on Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bank-additional-full.csv\",sep=\";\")\n",
    "sc = LabelEncoder()\n",
    "stdscl = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = {}\n",
    "df.iloc[:,-1] = sc.fit_transform(df.iloc[:,-1]).astype(float)\n",
    "col = df.loc[:,['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']]\n",
    "for i in col:\n",
    "    df.loc[:,i] = sc.fit_transform(df.loc[:,i])\n",
    "    embed[i] = df.loc[:,i]\n",
    "df.iloc[:,:-1] = stdscl.fit_transform(df.iloc[:,:-1])\n",
    "embed = pd.DataFrame(embed)\n",
    "embed = stdscl.fit_transform(embed)\n",
    "df = pd.DataFrame(df)\n",
    "embed = pd.DataFrame(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,nlayers,input, hidden,labels):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.n = nlayers\n",
    "        self.full_layer = {}\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last_layer = nn.Linear(hidden,labels)\n",
    "        for i in range(self.n):\n",
    "            if (i==0):\n",
    "                self.full_layer[\"fc{}\".format(i)] = nn.Linear(input, hidden)\n",
    "            else:\n",
    "                self.full_layer[\"fc{}\".format(i)] = nn.Linear(hidden, hidden)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for i in range(self.n):\n",
    "            output = self.full_layer[\"fc{}\".format(i)](X)\n",
    "            output = self.relu(output)\n",
    "        output = self.last_layer(output)\n",
    "        output = self.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold\n",
      "Fold No:0       Accuracy:96.25637290604516%\n",
      "Accuracy for each fold\n",
      "Fold No:1       Accuracy:94.05637701216403%\n",
      "Accuracy for each fold\n",
      "Fold No:2       Accuracy:75.89045087042028%\n"
     ]
    }
   ],
   "source": [
    "for fold,(train_id,test_id) in enumerate(kf.split(df)):\n",
    "    no_layers = 5\n",
    "    no_x = 20\n",
    "    no_hidden = 20\n",
    "    no_label = 2\n",
    "    epochs = 20\n",
    "    mu = 0.01\n",
    "    nn_model = NeuralNetwork(no_layers,no_x,no_hidden,no_label).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    df_tensor = torch.tensor(df.to_numpy())\n",
    "    ada_optimizer = Adam(nn_model.parameters(),lr=mu)\n",
    "    sampler_train = SubsetRandomSampler(train_id)\n",
    "    sampler_test = SubsetRandomSampler(test_id)\n",
    "    train_tensor = DataLoader(dataset=df_tensor,batch_size=20,sampler=sampler_train)\n",
    "    test_tensor = DataLoader(dataset=df_tensor,batch_size=20,sampler=sampler_test)\n",
    "    nn_model = nn_model.float()\n",
    "    writer = SummaryWriter()\n",
    "    total_loss = 0.0\n",
    "    for e in range(epochs):\n",
    "        #print(f'Epoch {e+1}/{epochs}')\n",
    "        for n,d in enumerate(train_tensor):\n",
    "            features,labels = d[:,:-1].float(),d[:,-1].long()\n",
    "            output = nn_model(features)\n",
    "            loss = loss_func(output,labels)\n",
    "            total_loss += loss\n",
    "            writer.add_scalar(\"Loss/train\", loss, n)\n",
    "            ada_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            ada_optimizer.step()\n",
    "    file = f'./{fold}-fold-model.pth'\n",
    "    torch.save(nn_model,file)\n",
    "    #nn_model = torch.load(f'./{fold}-fold-model.pth')\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    res = {}\n",
    "    total_loss_t = 0.0\n",
    "    with torch.no_grad():\n",
    "        for n,d in enumerate(test_tensor):\n",
    "            features,labels = d[:,:-1].float(),d[:,-1].long()\n",
    "            output = nn_model(features)\n",
    "            _,predict = torch.max(output.data,1)\n",
    "            loss = loss_func(output,labels)\n",
    "            writer.add_scalar(\"Loss/test\", loss, n)\n",
    "            total += labels.size(0)\n",
    "            correct += (predict==labels).sum().item()\n",
    "        res[fold] = (correct/total)*100\n",
    "    print(\"Accuracy for each fold\")\n",
    "    for i,j in res.items():\n",
    "        print(f'Fold No:{i}       Accuracy:{j}%')\n",
    "    writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Epoch for Training\n",
    "# ![Loss/Epoch for Training](tensorboard_fold_train.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Epoch for Testing\n",
    "# ![Loss/Epoch for Testing](tensorboard_fold_test.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NLP - Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_text.txt\",\"r\") as file:\n",
    "    text = file.read()\n",
    "words = text.strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "ln_layer = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_CBOW(nn.Module):\n",
    "    def __init__(self,no_words):\n",
    "        super(Neural_CBOW,self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=no_words,embedding_dim=embedding_size)\n",
    "        self.linear_layer = nn.Linear(embedding_size,ln_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,X):\n",
    "        output_cbow = self.embed(X)\n",
    "        output_cbow = self.linear_layer(X)\n",
    "        output_cbow = self.relu(X)\n",
    "        return output_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(words):\n",
    "    input_cbow = []\n",
    "    output_cbow = []\n",
    "    for w in range(0,len(words),5):\n",
    "        words[w].strip(\"\\n\")\n",
    "        if(w+4 < len(words)):\n",
    "            input_cbow.append(words[w])\n",
    "            input_cbow.append(words[w+1])\n",
    "            output_cbow.append(words[w+2])\n",
    "            input_cbow.append(words[w+3])\n",
    "            input_cbow.append(words[w+4])\n",
    "        else:\n",
    "            break\n",
    "    return input_cbow,output_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "data_cbow,targets_cbow = get_context(words)\n",
    "print(len(targets_cbow))\n",
    "data_cbow = sc.fit_transform(data_cbow)\n",
    "targets_cbow = sc.fit_transform(targets_cbow)\n",
    "data_cbow = np.array(data_cbow).reshape((len(data_cbow),1))\n",
    "targets_cbow = np.array(targets_cbow).reshape((len(targets_cbow),1))\n",
    "train_data_cbow = data_cbow[:29]\n",
    "test_data_cbow = data_cbow[29:58]\n",
    "data_cbow_t = np.append(train_data_cbow,targets_cbow,axis=1)\n",
    "data_cbow_tt = np.append(test_data_cbow,targets_cbow,axis=1)\n",
    "datas_tensor_train = torch.tensor(data_cbow_t)\n",
    "datas_tensor_test = torch.tensor(data_cbow_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Predator\\Documents\\ML Lab SEM4\\ML_LAB_Group1_311166_Exercise08.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Predator/Documents/ML%20Lab%20SEM4/ML_LAB_Group1_311166_Exercise08.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m nn_model \u001b[39m=\u001b[39m nn_model\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Predator/Documents/ML%20Lab%20SEM4/ML_LAB_Group1_311166_Exercise08.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Predator/Documents/ML%20Lab%20SEM4/ML_LAB_Group1_311166_Exercise08.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     features,label \u001b[39m=\u001b[39m train_cbow\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Predator/Documents/ML%20Lab%20SEM4/ML_LAB_Group1_311166_Exercise08.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     output \u001b[39m=\u001b[39m nn_model(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Predator/Documents/ML%20Lab%20SEM4/ML_LAB_Group1_311166_Exercise08.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(output,labels)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "cbow_model = Neural_CBOW(len(words)).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "ada_optimizer = Adam(nn_model.parameters(),lr=mu)\n",
    "train_cbow = DataLoader(dataset=datas_tensor_train,batch_size=5,shuffle=True)\n",
    "test_cbow = DataLoader(dataset=datas_tensor_test,batch_size=5,shuffle=True)\n",
    "nn_model = nn_model.float()\n",
    "for e in range(50):\n",
    "    features,label = train_cbow\n",
    "    output = nn_model(features)\n",
    "    loss = loss_func(output,labels)\n",
    "    ada_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    ada_optimizer.step()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    res = {}\n",
    "    total_loss_t = 0.0\n",
    "    with torch.no_grad():\n",
    "        features,labels = test_cbow\n",
    "        output = nn_model(features)\n",
    "        _,predict = torch.max(output.data,1)\n",
    "        loss = loss_func(output,labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict==labels).sum().item()\n",
    "    if(e%10 == 0):\n",
    "        print(f'Epoch {e}/{50} --> Accuracy: {(correct/total)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "2. https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n",
    "3. https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c12ac0d7e0ea61d5a4af92f561f297af769b3725060883501631cd3a01257e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
